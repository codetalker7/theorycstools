[["index.html", "Math/CS Theory Tools Chapter 1 Overview", " Math/CS Theory Tools Siddhant Chaudhary 2025-05-18 Chapter 1 Overview Please see a description of why I made this series of blogs on my website. I’m too lazy to write those points here again. "],["conditional-distributions.html", "Chapter 2 Conditional Distributions", " Chapter 2 Conditional Distributions [Sunday 18 May 2025 02:01:50 AM] Conditional distributions are one of the most important constructions of modern probability. However, there are multiple notions of it (which are similar, but need slightly different treatments mathematically, though most of them have the same essence). In this post, I’ll discuss some of those constructions (and book references where you can read more about them). Some of these abstractions can be really helpful if one comes across situations where naive notions of conditional probability might fail. The prerequisites for this post will be: Basic notions of measure theory; measures, measurable maps etc., and related concepts. The standard Borel \\(\\sigma\\)-algebra on \\(\\mathbb{R}\\). The Lebesgue integral, and some related convergence theorems (monotone, dominated convergence). Also, expected values. All these are standard notions studied in any measure theory course. I recommend the following books to understand them: Klenke, Achim. 2020. Probability Theory: A Comprehensive Course. 3rd ed. Springer. Bogachev, Vladimir I. 2007. Measure Theory, Volume 1. Springer. Rao, M. M. 2005. Conditional Measures and Applications. 2nd ed. Dudley, R. M. 2004. Real Analysis and Probability. "],["overview-1.html", "2.1 Overview", " 2.1 Overview We’ll look at three types of conditional distributions, depending on the type of objects we condition on: sets, \\(\\sigma\\)-algebras (or random variables), and sets of measure \\(0\\). Different applications require different levels of conditioning; in extreme cases, one may have to condition on impossible events for the sake of theoretical analysis. Typically, most courses on probability theory cover only the first level of conditioning (i.e conditioning on sets of positive measure). "],["conditioning-on-sets-of-positive-measure.html", "2.2 Conditioning on sets of positive measure", " 2.2 Conditioning on sets of positive measure This is probably the easiest and most intuitive notion of conditionals. Intuitively, this construction allows us to condition on events (or sets). Suppose \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) is a probability space, and let \\(B\\in\\mathcal{A}\\) be a set of positive measure (i.e., \\(\\mathbb{P}(B) &gt; 0\\)). In that case, we can define a new probability measure \\(\\mathbb{P}(\\cdot | B)\\) on \\(\\mathcal{A}\\) by the following (high school conditional measure): \\[ \\begin{aligned} \\mathbb{P}(A | B) := \\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)} \\end{aligned} \\] It is straightforward to verify that this is a probability measure on \\(\\mathcal{A}\\) . Note that, this notion of the conditional probability gives us a measure, and not a random variable, unlike some other notions of conditional expectations that we’ll see. "],["conditioning-on-sigma-algebras-and-random-variables.html", "2.3 Conditioning on \\(\\sigma\\)-algebras and random variables", " 2.3 Conditioning on \\(\\sigma\\)-algebras and random variables The next level of abstraction comes from a simple application: we want to condition on random variables (or, measurable maps), or more generally, \\(\\sigma\\)-algebras. Intuitively, suppose \\(X\\) and \\(Y\\) are random variables (in some probability space). What should conditioning \\(X\\) on \\(Y\\) mean? One way to think of it is this: if we’re given enough information about \\(Y\\), what is the best guess we can make for \\(X\\)? This guess is captured by the conditional expectation \\(\\mathbb{E}[X | Y]\\). Also, note that since we don’t condition on a specific value of \\(Y\\), we expect this object to itself be random. Let’s now see the more general conditional expectation, which conditions a random variable on a \\(\\sigma\\)-algebra. Let \\(X\\) be a random variable on a probability space \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\), and assume that \\(X\\in\\mathcal{L}^1(\\Omega, \\mathcal{A}, \\mathbb{P})\\) (i.e., \\(X\\) is Lebesgue integrable). Let \\(\\mathcal{F}\\subset\\mathcal{A}\\) be a sub-\\(\\sigma\\)-algebra. Again, the question we want to ask is: given \\(\\mathcal{F}\\), what is the best guess we can make about \\(X\\)? We’ll denote this guess by \\(\\mathbb{E}[X | \\mathcal{F}]\\). To that end, we posit that \\(\\mathbb{E}[X | \\mathcal{F}]\\) is itself a random variable. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
